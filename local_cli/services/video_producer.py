"""
Video Producer - ì˜ìƒ ì œì‘ ì„œë¹„ìŠ¤
"""
import os
import re
from typing import Dict, List, Tuple, Any, Optional
from .tts_service import TTSService
from .audio_processor import AudioProcessor
from .music_library import MusicLibrary
from .image_generator import ImageGenerator


class VideoProducer:
    """ì™„ì „í•œ ì˜ìƒ ì œì‘ íŒŒì´í”„ë¼ì¸

    TTS, ì˜¤ë””ì˜¤ ì²˜ë¦¬, ë¹„ì£¼ì–¼ ìƒì„±, ìë§‰, ì˜ìƒ í•©ì„±ì„ í¬í•¨í•œ
    ì „ì²´ ì˜ìƒ ì œì‘ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """

    # ì„ì‹œ ë¹„ì£¼ì–¼ ë°°ê²½ ìƒ‰ìƒ (AI ì´ë¯¸ì§€ ìƒì„± ë¹„í™œì„±í™” ì‹œ ì‚¬ìš©)
    VISUAL_COLORS = [
        (50, 50, 100),   # ì§„í•œ íŒŒë€ìƒ‰
        (100, 50, 50),   # ì§„í•œ ë¹¨ê°„ìƒ‰
        (50, 100, 50),   # ì§„í•œ ì´ˆë¡ìƒ‰
        (100, 100, 50),  # ë…¸ë€ìƒ‰
        (100, 50, 100),  # ë³´ë¼ìƒ‰
    ]

    def __init__(self):
        """VideoProducer ì´ˆê¸°í™”"""
        # ë¬´ë£Œ TTS ì‚¬ìš© (gTTS ë˜ëŠ” local)
        tts_provider = os.getenv('TTS_PROVIDER', 'gtts')
        self.tts_service = TTSService(provider=tts_provider)
        self.audio_processor = AudioProcessor()
        self.music_library = MusicLibrary()

        # ì´ë¯¸ì§€ ìƒì„± (Pexels API ê¸°ë³¸ í™œì„±í™”)
        # í™˜ê²½ ë³€ìˆ˜: pexels (ì¶”ì²œ), text (ê·¸ë¼ë°ì´ì…˜+í…ìŠ¤íŠ¸), unsplash, none
        image_provider = os.getenv('IMAGE_PROVIDER', 'pexels')
        self.image_generator = ImageGenerator(provider=image_provider)

    def produce_video(
        self,
        script: Dict,
        style_preset: str,
        output_path: str
    ) -> Tuple[str, str]:
        """ì™„ì „í•œ ì˜ìƒ ì œì‘ íŒŒì´í”„ë¼ì¸

        Args:
            script: ëŒ€ë³¸ ì •ë³´ (content, video_format í¬í•¨)
            style_preset: ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹ (calm, energetic ë“±)
            output_path: ì¶œë ¥ ì˜ìƒ ê²½ë¡œ (.mp4)

        Returns:
            Tuple[str, str]: (ì˜ìƒ ê²½ë¡œ, ì¸ë„¤ì¼ ê²½ë¡œ)

        Raises:
            Exception: ì˜ìƒ ì œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        """
        print("\nğŸ¬ ì˜ìƒ ì œì‘ ì‹œì‘...")

        temp_dir = './temp'
        os.makedirs(temp_dir, exist_ok=True)

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ë¯¸ë¦¬ ìƒì„±
        output_dir = os.path.dirname(output_path)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)

        # 1. TTS ìŒì„± ìƒì„±
        print("\n1ï¸âƒ£ ìŒì„± ìƒì„± ì¤‘...")
        voice_segments = self.tts_service.generate_with_timestamps(
            script['content'],
            output_dir=os.path.join(temp_dir, 'audio')
        )

        voice_path, voice_duration = self.audio_processor.merge_audio_segments(
            voice_segments,
            os.path.join(temp_dir, 'voice_final.mp3')
        )

        # 2. ë°°ê²½ìŒì•… ì¶”ê°€
        print("\n2ï¸âƒ£ ë°°ê²½ìŒì•… ì¶”ê°€ ì¤‘...")
        background_music_path = self.music_library.get_music_for_style(
            style_preset,
            int(voice_duration)
        )

        if background_music_path:
            adjusted_music_path = os.path.join(temp_dir, 'music_adjusted.mp3')
            self.audio_processor.adjust_audio_length(
                background_music_path,
                voice_duration,
                adjusted_music_path
            )

            final_audio_path = self.audio_processor.mix_voice_and_music(
                voice_path,
                adjusted_music_path,
                os.path.join(temp_dir, 'audio_with_music.mp3'),
                voice_volume=1.0,
                music_volume=0.25
            )
        else:
            final_audio_path = voice_path

        # 3. ì´ë¯¸ì§€/ì˜ìƒ í´ë¦½ ìƒì„±
        print("\n3ï¸âƒ£ ë¹„ì£¼ì–¼ ìƒì„± ì¤‘...")
        visual_clips = self._generate_visual_clips(
            script,
            voice_segments,
            style_preset
        )

        # 4. ìë§‰ ìƒì„±
        print("\n4ï¸âƒ£ ìë§‰ ìƒì„± ì¤‘...")
        subtitles = self._create_subtitles(voice_segments)

        # 5. ìµœì¢… í•©ì„±
        print("\n5ï¸âƒ£ ì˜ìƒ í•©ì„± ì¤‘...")
        final_video = self._compose_video(
            visual_clips,
            final_audio_path,
            subtitles,
            script['video_format'],
            voice_duration
        )

        final_video.write_videofile(
            output_path,
            fps=30,
            codec='libx264',
            audio_codec='aac',
            threads=4,
            preset='medium'
        )

        # 6. ì¸ë„¤ì¼ ìƒì„±
        print("\n6ï¸âƒ£ ì¸ë„¤ì¼ ìƒì„± ì¤‘...")
        thumbnail_path = output_path.replace('.mp4', '_thumb.jpg')

        # RGBAë¥¼ RGBë¡œ ë³€í™˜í•˜ì—¬ JPEG ì €ì¥
        from PIL import Image
        import numpy as np

        frame = final_video.get_frame(2)  # 2ì´ˆ ì‹œì  í”„ë ˆì„
        img = Image.fromarray(frame)

        # RGBAì¸ ê²½ìš° RGBë¡œ ë³€í™˜
        if img.mode == 'RGBA':
            img = img.convert('RGB')

        img.save(thumbnail_path, 'JPEG', quality=95)

        print(f"\nâœ… ì˜ìƒ ìƒì„± ì™„ë£Œ: {output_path}")

        return output_path, thumbnail_path

    def _generate_visual_clips(
        self,
        script: Dict,
        voice_segments: List[Dict],
        style_preset: str
    ) -> List[Any]:
        """ë¹„ì£¼ì–¼ í´ë¦½ ìƒì„±

        AI ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œë„í•˜ë©°, ì‹¤íŒ¨ ì‹œ ë‹¨ìƒ‰ ë°°ê²½ ì‚¬ìš©

        Args:
            script: ëŒ€ë³¸ ì •ë³´
            voice_segments: TTS ì„¸ê·¸ë¨¼íŠ¸ (duration í¬í•¨)
            style_preset: ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹

        Returns:
            List[MoviePy VideoClip]: ë¹„ì£¼ì–¼ í´ë¦½ ë¦¬ìŠ¤íŠ¸

        Raises:
            ImportError: MoviePyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
        """
        try:
            # MoviePy 2.x import
            from moviepy import (
                VideoFileClip, ImageClip, ColorClip,
                concatenate_videoclips, CompositeVideoClip,
                AudioFileClip, TextClip
            )
        except ImportError:
            raise ImportError(
                "moviepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜: pip install moviepy imageio-ffmpeg"
            )

        clips = []

        # ì „ì²´ ì˜ìƒ ê¸¸ì´ ê³„ì‚°
        total_duration = sum(seg.get('duration', 5.0) for seg in voice_segments)

        # ìµœì†Œ 5ê°œ ì´ë¯¸ì§€ ë³´ì¥
        min_images = 5
        num_segments = len(voice_segments)

        # ì´ë¯¸ì§€ ë³€ê²½ ê°„ê²© ê³„ì‚° (ìµœì†Œ 3ì´ˆ, ìµœëŒ€ 10ì´ˆ)
        if num_segments >= min_images:
            # ì„¸ê·¸ë¨¼íŠ¸ê°€ ì¶©ë¶„í•˜ë©´ ê° ì„¸ê·¸ë¨¼íŠ¸ë‹¹ 1ê°œ ì´ë¯¸ì§€
            image_change_points = []
            current_time = 0.0
            for segment in voice_segments:
                image_change_points.append((current_time, segment['text']))
                current_time += segment.get('duration', 5.0)
        else:
            # ì„¸ê·¸ë¨¼íŠ¸ê°€ ë¶€ì¡±í•˜ë©´ ê· ë“±í•˜ê²Œ ë¶„í•  (ìµœì†Œ 5ê°œ)
            image_interval = max(3.0, min(10.0, total_duration / min_images))
            image_change_points = []
            current_time = 0.0
            seg_idx = 0

            while current_time < total_duration and len(image_change_points) < min_images * 2:  # ë„‰ë„‰í•˜ê²Œ
                # í˜„ì¬ ì‹œê°„ì— í•´ë‹¹í•˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ ì°¾ê¸°
                time_sum = 0.0
                for idx, segment in enumerate(voice_segments):
                    time_sum += segment.get('duration', 5.0)
                    if time_sum > current_time:
                        seg_idx = idx
                        break

                if seg_idx < len(voice_segments):
                    image_change_points.append((current_time, voice_segments[seg_idx]['text']))

                current_time += image_interval

        # ìµœì†Œ 5ê°œ ë³´ì¥
        if len(image_change_points) < min_images:
            # ë¶€ì¡±í•˜ë©´ ì„¸ê·¸ë¨¼íŠ¸ í…ìŠ¤íŠ¸ ë°˜ë³µ ì‚¬ìš©
            while len(image_change_points) < min_images:
                idx = len(image_change_points) % num_segments
                time_point = (len(image_change_points) * total_duration / min_images,
                             voice_segments[idx]['text'])
                image_change_points.append(time_point)

        print(f"\n[INFO] ì´ {len(image_change_points)}ê°œ ì´ë¯¸ì§€ ìƒì„± ì˜ˆì • (ìµœì†Œ {min_images}ê°œ ë³´ì¥)")

        # AI ì´ë¯¸ì§€ ìƒì„±
        if self.image_generator.enabled:
            print("\nğŸ¨ AI ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
            image_paths = []
            for i, (time_point, text) in enumerate(image_change_points):
                output_path = f'./temp/images/image_{i}.png'
                image_path = self.image_generator.generate_image_for_segment(
                    text,
                    style_preset,
                    output_path
                )
                image_paths.append(image_path)
        else:
            image_paths = [None] * len(image_change_points)

        # ì´ë¯¸ì§€ í´ë¦½ ìƒì„±
        for i, ((start_time, text), image_path) in enumerate(zip(image_change_points, image_paths)):
            # ë‹¤ìŒ ì´ë¯¸ì§€ ë³€ê²½ ì‹œì ê¹Œì§€ì˜ duration ê³„ì‚°
            if i < len(image_change_points) - 1:
                duration = image_change_points[i + 1][0] - start_time
            else:
                duration = total_duration - start_time

            # durationì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ìŒìˆ˜ë©´ ë³´ì •
            if duration <= 0:
                continue

            # AI ìƒì„± ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë‹¨ìƒ‰ ë°°ê²½
            if image_path and os.path.exists(image_path):
                clip = ImageClip(image_path, duration=duration)
                print(f"[OK] AI ì´ë¯¸ì§€ ì‚¬ìš©: {os.path.basename(image_path)} ({duration:.1f}ì´ˆ)")
            else:
                # PILë¡œ ë‹¨ìƒ‰ ë°°ê²½ ì´ë¯¸ì§€ ìƒì„±
                from PIL import Image
                import numpy as np

                color = self.VISUAL_COLORS[i % len(self.VISUAL_COLORS)]
                img = Image.new('RGB', (1920, 1080), color)
                img_array = np.array(img)

                clip = ImageClip(img_array, duration=duration)

            # ì¤Œ íš¨ê³¼
            clip = clip.resized(lambda t: 1 + 0.01 * t)

            clips.append(clip)

        return clips

    def _create_subtitles(self, voice_segments: List[Dict]) -> List[Dict]:
        """ìë§‰ ë°ì´í„° ìƒì„± (TTS ì„¸ê·¸ë¨¼íŠ¸ ì‹¤ì œ ê¸¸ì´ ì‚¬ìš©)"""
        subtitle_data = []

        cumulative_time = 0.0
        for segment in voice_segments:
            # ì„¸ê·¸ë¨¼íŠ¸ ì‹¤ì œ ê¸¸ì´ ì‚¬ìš© (ê¸°ë³¸ê°’ 5ì´ˆ)
            duration = segment.get('duration', 5.0)

            subtitle_data.append({
                'start': cumulative_time,
                'end': cumulative_time + duration,
                'text': segment['text']
            })

            cumulative_time += duration

        return subtitle_data

    def _find_font(self) -> Optional[str]:
        """ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ì°¾ê¸°

        Returns:
            Optional[str]: í°íŠ¸ ê²½ë¡œ ë˜ëŠ” None (ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©)
        """
        import platform

        # Windows í°íŠ¸ ê²½ë¡œ
        if platform.system() == 'Windows':
            fonts_dir = r'C:\Windows\Fonts'

            # í•œê¸€ ì§€ì› í°íŠ¸ ìš°ì„  (ë§‘ì€ ê³ ë”•)
            preferred_fonts = [
                os.path.join(fonts_dir, 'malgun.ttf'),     # ë§‘ì€ ê³ ë”•
                os.path.join(fonts_dir, 'malgunbd.ttf'),   # ë§‘ì€ ê³ ë”• Bold
                os.path.join(fonts_dir, 'gulim.ttc'),      # êµ´ë¦¼
                os.path.join(fonts_dir, 'arial.ttf'),      # Arial
                os.path.join(fonts_dir, 'arialbd.ttf'),    # Arial Bold
            ]
        # Linux í°íŠ¸ ê²½ë¡œ
        elif platform.system() == 'Linux':
            preferred_fonts = [
                '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
                '/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf',
                '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
            ]
        # macOS í°íŠ¸ ê²½ë¡œ
        elif platform.system() == 'Darwin':
            preferred_fonts = [
                '/System/Library/Fonts/AppleSDGothicNeo.ttc',
                '/Library/Fonts/Arial Bold.ttf',
                '/System/Library/Fonts/Helvetica.ttc',
            ]
        else:
            preferred_fonts = []

        # ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ í°íŠ¸ ë°˜í™˜
        for font_path in preferred_fonts:
            if os.path.exists(font_path):
                print(f"âœ… í°íŠ¸ ë°œê²¬: {font_path}")
                return font_path

        # í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
        print("âš ï¸ ì‹œìŠ¤í…œ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")
        return None

    def _compose_video(
        self,
        visual_clips: List,
        audio_path: str,
        subtitles: List[Dict],
        video_format: str,
        duration: float
    ):
        """ìµœì¢… ì˜ìƒ í•©ì„±"""

        try:
            # MoviePy 2.x import
            from moviepy import (
                concatenate_videoclips, CompositeVideoClip,
                AudioFileClip, TextClip
            )
        except ImportError:
            raise ImportError("moviepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install moviepy imageio-ffmpeg")

        # ë¹„ì£¼ì–¼ ì—°ê²°
        video = concatenate_videoclips(visual_clips, method="compose")

        # ì˜ìƒì„ ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ì¶¤
        if video.duration > duration:
            video = video.subclipped(0, duration)
        elif video.duration < duration:
            # ë§ˆì§€ë§‰ í”„ë ˆì„ì„ freeze
            last_frame = visual_clips[-1]
            video = concatenate_videoclips([video, last_frame.with_duration(duration - video.duration)])

        # ì˜¤ë””ì˜¤ ì¶”ê°€
        audio = AudioFileClip(audio_path)
        video = video.with_audio(audio)

        # ìˆí¼ì€ 9:16 í¬ë¡­ (ìë§‰ ì¶”ê°€ ì „ì— ë¨¼ì € í¬ë¡­)
        if video_format == 'short':
            video = video.cropped(
                x_center=int(video.w/2),
                y_center=int(video.h/2),
                width=int(video.h * 9/16),
                height=int(video.h)
            )

        # ìë§‰ ì¶”ê°€ (í°íŠ¸ ê²½ë¡œ ìë™ íƒì§€)
        font_path = self._find_font()

        def make_textclip(txt):
            # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ í°íŠ¸ í¬ê¸° ë™ì  ì¡°ì ˆ
            text_length = len(txt)

            if video_format == 'short':
                # ìˆí¼: ë” ê³µê²©ì ì¸ í°íŠ¸ í¬ê¸° ì¡°ì ˆë¡œ ì˜ë¦¼ ë°©ì§€
                if text_length < 20:
                    font_size = 42
                elif text_length < 35:
                    font_size = 35
                elif text_length < 50:
                    font_size = 30
                elif text_length < 70:
                    font_size = 26
                else:
                    font_size = 22  # ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸
            else:
                # ê¸´ ì˜ìƒ: ë” ê³µê²©ì ì¸ í°íŠ¸ í¬ê¸° ì¡°ì ˆ
                if text_length < 25:
                    font_size = 45
                elif text_length < 45:
                    font_size = 38
                elif text_length < 65:
                    font_size = 32
                elif text_length < 90:
                    font_size = 28
                else:
                    font_size = 24  # ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸

            # ìë§‰ ì˜ì—­ì„ 95%ë¡œ í™•ëŒ€ (ë” ë„“ê²Œ)
            max_width = int(video.w * 0.95)

            return TextClip(
                text=txt,
                font=font_path,
                font_size=font_size,
                color='white',
                stroke_color='black',
                stroke_width=2,
                method='caption',
                size=(max_width, None)  # 90%ë¡œ í™•ëŒ€
            )

        subtitle_clips = []
        for sub in subtitles:
            txt_clip = make_textclip(sub['text'])
            txt_clip = txt_clip.with_start(sub['start']).with_duration(sub['end'] - sub['start'])

            # ìˆí¼ì€ í™”ë©´ ì¤‘ì•™ í•˜ë‹¨, ê¸´ ì˜ìƒì€ í•˜ë‹¨
            if video_format == 'short':
                txt_clip = txt_clip.with_position(('center', int(video.h * 0.75)))  # 75% ìœ„ì¹˜
            else:
                txt_clip = txt_clip.with_position(('center', int(video.h * 0.85)))  # 85% ìœ„ì¹˜

            subtitle_clips.append(txt_clip)

        video = CompositeVideoClip([video] + subtitle_clips)

        return video
